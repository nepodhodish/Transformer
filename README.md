# Custom Transformer with Multivariable Input & Parallel Training (FSDP) 	
Independent AI Project 		April 2025 – Present
 - Designed and implemented Transformer architecture from scratch to process multivariable time series inputs. 
 - Integrated rotational positional encoding, masked self-attention, and flexible embedding layers using PyTorch. 
 - Parallelized training across multiple GPUs using Fully Sharded Data Parallel (FSDP) to reduce memory usage and increase batch size. 
 - Tuned training stability and runtime efficiency using cutting-edge approaches in weights’ initialization and mixed precision. 
 - Built complete pipeline: model, data loader, training loop, and reproducibility utilities.
